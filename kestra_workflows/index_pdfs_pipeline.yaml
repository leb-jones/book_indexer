id: index-pdfs-pipeline
namespace: indexing

tasks:
  # ðŸ”¹ Upload PDFs to Existing GCS Bucket
  - id: upload-pdfs
    type: io.kestra.plugin.gcp.gcs.Upload
    bucket: indexing-pdf-storage
    from: /home/lebjones/PDFIndexer/pdfs/
    to: pdfs/
    move: false

  # ðŸ”¹ Run the Indexing Script in Dataproc
  - id: run-dataproc-job
    type: io.kestra.plugin.scripts.bash.Commands
    commands:
      - gcloud dataproc jobs submit pyspark gs://indexing-pdf-storage/scripts/indexer.py \
          --cluster=pdf-indexing-cluster \
          --region=us-central1 \
          --jars gs://hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar \
          --properties google.cloud.auth.service.account.json.keyfile=$DATAPROC_KEY_PATH

  # ðŸ”¹ Trigger dbt Cloud Transformation (No Duplication of Table Creation)
  - id: trigger-dbt-cloud
    type: io.kestra.plugin.dbt.cloud.RunJob
    accountId: "your-dbt-cloud-account-id"
    jobId: "your-dbt-job-id"
    token: "your-dbt-cloud-api-token"

  # ðŸ”¹ Wait for dbt Cloud to Finish
  - id: check-dbt-status
    type: io.kestra.plugin.dbt.cloud.WaitForRun
    accountId: "your-dbt-cloud-account-id"
    runId: "{{ outputs.trigger-dbt-cloud.runId }}"
    token: "your-dbt-cloud-api-token"

  # ðŸ”¹ Cleanup GCS Bucket (Remove PDFs After Processing)
  - id: cleanup-gcs
    type: io.kestra.plugin.scripts.bash.Commands
    commands:
      - gcloud storage rm --recursive gs://indexing-pdf-storage/pdfs/

  # ðŸ”¹ (Optional) Delete Dataproc Cluster After Job Completion
  - id: delete-dataproc-cluster
    type: io.kestra.plugin.scripts.bash.Commands
    commands:
      - gcloud dataproc clusters delete pdf-indexing-cluster --region=us-central1 --quiet
